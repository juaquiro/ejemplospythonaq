{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0480183f",
   "metadata": {},
   "source": [
    "\n",
    "# CPU vs GPU Inference Benchmark (PyTorch)\n",
    "\n",
    "This notebook measures **inference performance** on **CPU vs GPU** using PyTorch and a standard CNN backbone (`resnet18`).  \n",
    "It is designed to run **both on your local machine** (with or without GPU) and **in Google Colab** (enable GPU via *Runtime → Change runtime type → GPU*).\n",
    "\n",
    "**What you'll see**\n",
    "- Device detection (CPU/GPU/TPU).\n",
    "- Timed inference runs over increasing batch sizes.\n",
    "- Per-image latency and throughput comparisons.\n",
    "- Simple plots showing how GPUs scale with batch size.\n",
    "\n",
    "> Tip: If you run locally without a GPU, you can still run the CPU part and then compare with Colab-GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaeb007",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup\n",
    "If PyTorch/torchvision are missing, uncomment and run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install --upgrade pip\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# If you have a CUDA-capable local machine and want a CUDA build, see: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62134608",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Imports & Device Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971dc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running without CUDA (CPU only).\")\n",
    "\n",
    "# Colab TPU (rare for this demo) - not used here but we note it.\n",
    "try:\n",
    "    import torch_xla\n",
    "    print(\"TPU environment detected (XLA). This notebook focuses on CPU/GPU.\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5922241",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Build Model\n",
    "We use `resnet18` without pretrained weights to avoid downloads. For inference timing, weights are irrelevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: ResNet18 | Parameters: {total_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a3d6c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Benchmark Helpers\n",
    "We benchmark multiple batch sizes, with warm-up and repeated timed runs, computing:\n",
    "- **Latency per image (ms/img)**\n",
    "- **Throughput (images/sec)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d577111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark_inference(model, device, batch_sizes=(1, 8, 32, 64, 128, 256), \n",
    "                        input_size=(3, 224, 224), warmup=10, iters=30):\n",
    "    model.to(device)\n",
    "    results = []\n",
    "    for bs in batch_sizes:\n",
    "        x = torch.randn(bs, *input_size, device=device)\n",
    "\n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(warmup):\n",
    "                _ = model(x)\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "        # Timed runs\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(iters):\n",
    "                _ = model(x)\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        total_images = bs * iters\n",
    "        elapsed = t1 - t0\n",
    "        throughput = total_images / elapsed\n",
    "        latency_ms_per_img = (elapsed / total_images) * 1000.0\n",
    "        results.append({\n",
    "            \"batch_size\": bs,\n",
    "            \"elapsed_s\": elapsed,\n",
    "            \"throughput_img_s\": throughput,\n",
    "            \"latency_ms_per_img\": latency_ms_per_img\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021db11a",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Run CPU Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "cpu_results = benchmark_inference(model, device_cpu)\n",
    "cpu_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fc017",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Run GPU Benchmark (if available)\n",
    "Enable GPU in Colab via **Runtime → Change runtime type → GPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu_results = None\n",
    "if torch.cuda.is_available():\n",
    "    device_gpu = torch.device(\"cuda:0\")\n",
    "    gpu_results = benchmark_inference(model, device_gpu)\n",
    "gpu_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0f4be",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Aggregate Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_arrays(results):\n",
    "    bs = [r[\"batch_size\"] for r in results]\n",
    "    thr = [r[\"throughput_img_s\"] for r in results]\n",
    "    lat = [r[\"latency_ms_per_img\"] for r in results]\n",
    "    return np.array(bs), np.array(thr), np.array(lat)\n",
    "\n",
    "cpu_bs, cpu_thr, cpu_lat = to_arrays(cpu_results)\n",
    "if gpu_results:\n",
    "    gpu_bs, gpu_thr, gpu_lat = to_arrays(gpu_results)\n",
    "else:\n",
    "    gpu_bs = gpu_thr = gpu_lat = None\n",
    "\n",
    "print(\"CPU throughput (img/s):\", cpu_thr)\n",
    "print(\"CPU latency (ms/img):\", cpu_lat)\n",
    "if gpu_thr is not None:\n",
    "    print(\"GPU throughput (img/s):\", gpu_thr)\n",
    "    print(\"GPU latency (ms/img):\", gpu_lat)\n",
    "else:\n",
    "    print(\"GPU not available; run in Colab with GPU to compare.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ca9fd",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Plot: Throughput vs Batch Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(cpu_bs, cpu_thr, marker='o', label='CPU')\n",
    "if gpu_thr is not None:\n",
    "    plt.plot(gpu_bs, gpu_thr, marker='o', label='GPU')\n",
    "plt.title(\"Throughput vs Batch Size (higher is better)\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Images per second\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c22bf",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Plot: Per-Image Latency vs Batch Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(cpu_bs, cpu_lat, marker='o', label='CPU')\n",
    "if gpu_lat is not None:\n",
    "    plt.plot(gpu_bs, gpu_lat, marker='o', label='GPU')\n",
    "plt.title(\"Latency per Image vs Batch Size (lower is better)\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Latency (ms per image)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511d712",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Summary Table\n",
    "Per-image latency and throughput for each device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17895c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_cpu = pd.DataFrame(cpu_results)\n",
    "df_cpu[\"device\"] = \"CPU\"\n",
    "\n",
    "if gpu_results:\n",
    "    df_gpu = pd.DataFrame(gpu_results)\n",
    "    df_gpu[\"device\"] = \"GPU\"\n",
    "    df = pd.concat([df_cpu, df_gpu], ignore_index=True)\n",
    "else:\n",
    "    df = df_cpu\n",
    "\n",
    "# Display as a user-visible table if running in a compatible environment\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"CPU vs GPU Inference Results\", df)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "df.round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5f82e",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Notes & Interpretation\n",
    "- **GPU wins on larger batch sizes** due to parallelism, achieving much higher throughput.\n",
    "- **CPU latency** for small batches can be competitive, but **GPU** often dominates for medium/large batches.\n",
    "- Results vary with: GPU/CPU model, PyTorch build, drivers, background load.\n",
    "\n",
    "### Customize\n",
    "- Change `batch_sizes` in `benchmark_inference` (e.g., `(1, 4, 8, 16, 32, 64, 128, 256, 512)`).\n",
    "- Increase `iters` for more stable timing (will take longer).\n",
    "- Swap `resnet18` for `resnet50` or `vit_b_16` to stress hardware more.\n",
    "- Try `torch.backends.cudnn.benchmark = True` for conv-heavy models (may improve performance on fixed shapes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812daf0",
   "metadata": {},
   "source": [
    "\n",
    "## 12) (Optional) TensorFlow Variant\n",
    "If you prefer TensorFlow, you can adapt the same logic: build a Keras model, run warmups, and time multiple batches on CPU/GPU.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

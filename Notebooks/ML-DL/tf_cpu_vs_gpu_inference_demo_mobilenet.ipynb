{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61eeeeeb",
   "metadata": {},
   "source": [
    "\n",
    "# CPU vs GPU Inference Benchmark (TensorFlow 2 / Keras) — ResNet50 & MobileNetV2\n",
    "\n",
    "This notebook measures **inference performance** on **CPU vs GPU** using TensorFlow 2.  \n",
    "You can choose between two popular CNN backbones:\n",
    "- **ResNet50** (heavier, ~25M params — stresses GPU more)\n",
    "- **MobileNetV2** (lightweight, ~3.5M params — runs fast on CPU too)\n",
    "\n",
    "Run locally (with or without GPU) and in **Google Colab** (enable GPU via *Runtime → Change runtime type → GPU*).\n",
    "\n",
    "**What you'll see**\n",
    "- Device detection (CPU/GPU).\n",
    "- Timed inference runs over increasing batch sizes.\n",
    "- Per-image latency and throughput comparisons.\n",
    "- Plots showing scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92781d6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup (optional)\n",
    "TensorFlow is usually present on Colab. Locally, install a matching TF build.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment if you need to install locally (CPU build example):\n",
    "# !pip install --upgrade pip\n",
    "# !pip install tensorflow==2.15.*\n",
    "# For NVIDIA GPU locally, install CUDA/cuDNN compatible with your TF version per https://www.tensorflow.org/install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42a692",
   "metadata": {},
   "source": [
    "## 2) Imports & Device Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7059834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for d in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(d, True)\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555096a1",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Choose Model\n",
    "Set `MODEL_NAME` to `\"ResNet50\"` or `\"MobileNetV2\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f36df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
    "\n",
    "MODEL_NAME = \"MobileNetV2\"  # options: \"ResNet50\", \"MobileNetV2\"\n",
    "INPUT_SHAPE = (224, 224, 3)  # keep same for fair comparison\n",
    "\n",
    "def build_model(name=\"ResNet50\", input_shape=(224,224,3), num_classes=1000):\n",
    "    inp = Input(shape=input_shape)\n",
    "    if name == \"ResNet50\":\n",
    "        base = ResNet50(include_top=True, weights=None, input_tensor=inp, classes=num_classes)\n",
    "    elif name == \"MobileNetV2\":\n",
    "        base = MobileNetV2(include_top=True, weights=None, input_tensor=inp, classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model name: \" + str(name))\n",
    "    model = Model(inputs=base.input, outputs=base.output)\n",
    "    return model\n",
    "\n",
    "model = build_model(MODEL_NAME, INPUT_SHAPE, 1000)\n",
    "model.trainable = False\n",
    "model.compile()  # trivial compile to enable graphing if needed\n",
    "\n",
    "total_params = model.count_params()\n",
    "print(f\"Model: {MODEL_NAME} | Parameters: {total_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5160a7",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Benchmark Helpers\n",
    "We benchmark multiple batch sizes with warm-up and repeated timed runs.  \n",
    "Metrics:\n",
    "- **Latency per image (ms/img)**\n",
    "- **Throughput (images/sec)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function(jit_compile=False)\n",
    "def forward_pass(model, x):\n",
    "    return model(x, training=False)\n",
    "\n",
    "def benchmark_inference(model, device_str, batch_sizes=(1, 8, 16, 32, 64, 128), \n",
    "                        input_size=(224, 224, 3), warmup=10, iters=30):\n",
    "    results = []\n",
    "    for bs in batch_sizes:\n",
    "        with tf.device(device_str):\n",
    "            x = tf.random.normal([bs, *input_size], dtype=tf.float32)\n",
    "\n",
    "            # Warmup\n",
    "            for _ in range(warmup):\n",
    "                _ = forward_pass(model, x)\n",
    "\n",
    "            # Timed runs\n",
    "            t0 = time.perf_counter()\n",
    "            for _ in range(iters):\n",
    "                _ = forward_pass(model, x)\n",
    "            tf.experimental.sync_devices()  # ensure all device ops complete\n",
    "            t1 = time.perf_counter()\n",
    "\n",
    "        total_images = bs * iters\n",
    "        elapsed = t1 - t0\n",
    "        throughput = total_images / elapsed\n",
    "        latency_ms_per_img = (elapsed / total_images) * 1000.0\n",
    "        results.append({\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"batch_size\": bs,\n",
    "            \"elapsed_s\": elapsed,\n",
    "            \"throughput_img_s\": throughput,\n",
    "            \"latency_ms_per_img\": latency_ms_per_img,\n",
    "            \"device\": \"GPU\" if \"GPU\" in device_str else \"CPU\"\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddb102",
   "metadata": {},
   "source": [
    "## 5) Run CPU Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpu_device = \"/CPU:0\"\n",
    "cpu_results = benchmark_inference(model, cpu_device, input_size=INPUT_SHAPE)\n",
    "cpu_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea84b19",
   "metadata": {},
   "source": [
    "## 6) Run GPU Benchmark (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu_results = None\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_device = \"/GPU:0\"\n",
    "    gpu_results = benchmark_inference(model, gpu_device, input_size=INPUT_SHAPE)\n",
    "gpu_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7cf87b",
   "metadata": {},
   "source": [
    "## 7) Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def to_arrays(results):\n",
    "    bs = [r[\"batch_size\"] for r in results]\n",
    "    thr = [r[\"throughput_img_s\"] for r in results]\n",
    "    lat = [r[\"latency_ms_per_img\"] for r in results]\n",
    "    return np.array(bs), np.array(thr), np.array(lat)\n",
    "\n",
    "cpu_bs, cpu_thr, cpu_lat = to_arrays(cpu_results)\n",
    "gpu_bs = gpu_thr = gpu_lat = None\n",
    "if gpu_results:\n",
    "    gpu_bs, gpu_thr, gpu_lat = to_arrays(gpu_results)\n",
    "\n",
    "print(\"CPU throughput (img/s):\", cpu_thr)\n",
    "print(\"CPU latency (ms/img):\", cpu_lat)\n",
    "if gpu_thr is not None:\n",
    "    print(\"GPU throughput (img/s):\", gpu_thr)\n",
    "    print(\"GPU latency (ms/img):\", gpu_lat)\n",
    "else:\n",
    "    print(\"No GPU found. Run in Colab with GPU for comparison.\")\n",
    "\n",
    "df_cpu = pd.DataFrame(cpu_results)\n",
    "df = df_cpu.copy()\n",
    "if gpu_results:\n",
    "    df_gpu = pd.DataFrame(gpu_results)\n",
    "    df = pd.concat([df_cpu, df_gpu], ignore_index=True)\n",
    "\n",
    "# Display as a user-visible table when possible\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"TF CPU vs GPU Inference Results\", df)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "df.round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d7615",
   "metadata": {},
   "source": [
    "## 8) Plot: Throughput vs Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bff275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(cpu_bs, cpu_thr, marker='o', label='CPU')\n",
    "if gpu_bs is not None:\n",
    "    plt.plot(gpu_bs, gpu_thr, marker='o', label='GPU')\n",
    "plt.title(f\"{MODEL_NAME}: Throughput vs Batch Size (higher is better)\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Images per second\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1138d9",
   "metadata": {},
   "source": [
    "## 9) Plot: Per-Image Latency vs Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4acff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(cpu_bs, cpu_lat, marker='o', label='CPU')\n",
    "if gpu_bs is not None:\n",
    "    plt.plot(gpu_bs, gpu_lat, marker='o', label='GPU')\n",
    "plt.title(f\"{MODEL_NAME}: Latency per Image vs Batch Size (lower is better)\")\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Latency (ms per image)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4f94a",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Notes & Tips\n",
    "- **Choose model** by setting `MODEL_NAME` to `\"MobileNetV2\"` (fast/local) or `\"ResNet50\"` (heavier, stresses GPU).\n",
    "- **Graph mode** (`@tf.function`) is enabled for the forward pass to reduce Python overhead.\n",
    "- Results vary with hardware, TF build, drivers, and background load.\n",
    "- To stress hardware more, increase `iters` or batch sizes, or switch to a larger network.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

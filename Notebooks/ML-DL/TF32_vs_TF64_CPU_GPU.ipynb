{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Benchmark: float32 vs float64 on CPU and GPU\n",
    "\n",
    "This notebook measures the performance difference between **float32** and **float64** on **CPU** and (if available) **GPU** using several workloads:\n",
    "\n",
    "- Large matrix multiplication (GEMM)\n",
    "- 2D convolution\n",
    "- Inference through a dense neural network\n",
    "\n",
    "It reports per-iteration time, medians, and a summary table, and plots a simple bar chart.\n",
    "\n",
    "## How to use\n",
    "1. **Install/verify TensorFlow 2.x** in your environment.\n",
    "2. If you're in **Google Colab**, go to **Runtime → Change runtime type → GPU** to enable a GPU.\n",
    "3. Run all cells. The notebook will auto-detect devices and only run GPU tests if a GPU is present.\n",
    "4. You can tweak problem sizes in the configuration cell to match your hardware.\n",
    "\n",
    "## Notes\n",
    "- GPU support for float64 is hardware and driver dependent. Many consumer GPUs run float64 slowly compared to float32.\n",
    "- For a fair comparison, each test does a short warm-up before timing.\n",
    "- Results can vary due to background processes and TensorFlow graph optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Python: 3.10.18\n",
      "NumPy: 1.26.4\n",
      "CPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import os, time, math, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorFlow is not installed in this environment. Please install TF 2.x and retry.\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "\n",
    "# List devices\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"CPUs:\", cpus)\n",
    "print(\"GPUs:\", gpus)\n",
    "\n",
    "# Create logical GPU if present (optional, keeps default behavior)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "You can reduce sizes if you get out-of-memory errors, or increase them for more stable timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/CPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem sizes (adjust to your hardware)\n",
    "MATMUL_N = 4096      # MatMul size: N x N\n",
    "CONV_B = 32          # batch size\n",
    "CONV_H = 256         # height\n",
    "CONV_W = 256         # width\n",
    "CONV_CIN = 32        # input channels\n",
    "CONV_COUT = 64       # output channels (filters)\n",
    "KERNEL = 3           # kernel size\n",
    "\n",
    "DENSE_IN = 4096      # dense model input features\n",
    "DENSE_H = 512        # hidden units per layer\n",
    "DENSE_L = 4          # number of hidden layers\n",
    "DENSE_OUT = 1024     # output features\n",
    "DENSE_B = 1024       # batch size for inference\n",
    "\n",
    "# Benchmark parameters\n",
    "WARMUP = 5\n",
    "ITERS = 20\n",
    "\n",
    "# Dtypes and devices to test\n",
    "DTYPES = [tf.float32, tf.float64]\n",
    "DEVICES = ['/CPU:0'] + (['/GPU:0'] if tf.config.list_physical_devices('GPU') else [])\n",
    "DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sync(t):\n",
    "    \"\"\"Force device synchronization in all TF versions by fetching a scalar.\"\"\"\n",
    "    try:\n",
    "        # Reduce to a scalar on device, then copy to host -> blocks until done\n",
    "        _ = tf.reduce_sum(t).numpy()\n",
    "    except Exception:\n",
    "        # If t isn't a Tensor (or it's a tuple/list), be defensive\n",
    "        if isinstance(t, (list, tuple)) and t:\n",
    "            _ = tf.reduce_sum(t[0]).numpy()\n",
    "        else:\n",
    "            # Last resort: make a tiny op and fetch it\n",
    "            _ = tf.constant(0).numpy()\n",
    "\n",
    "def benchmark_op(op_fn, warmup=WARMUP, iters=ITERS):\n",
    "    \"\"\"Run op_fn() warmup times, then iters times, return a list of elapsed seconds.\"\"\"\n",
    "    # Warm-up (also builds @tf.function graphs)\n",
    "    for _ in range(warmup):\n",
    "        y = op_fn()\n",
    "        _sync(y)\n",
    "\n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        y = op_fn()\n",
    "        _sync(y)  # <- replaces tf.experimental.async_wait()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    return times\n",
    "\n",
    "def median(x):\n",
    "    x = sorted(x)\n",
    "    n = len(x)\n",
    "    return x[n//2] if n % 2 else 0.5*(x[n//2-1]+x[n//2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload 1: Large Matrix Multiplication (GEMM)\n",
    "Computes `C = A @ B` where A and B are `N x N`. This is compute-bound and often shows clear GPU advantages for float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workload': 'matmul', 'device': '/CPU:0', 'dtype': 'float32', 'iters': 20, 'time_median_s': 0.13776399999915157, 'time_mean_s': 0.13807563999871492, 'time_std_s': 0.003223546195631514}\n",
      "{'workload': 'matmul', 'device': '/CPU:0', 'dtype': 'float64', 'iters': 20, 'time_median_s': 0.35059370000089984, 'time_mean_s': 0.352023080001527, 'time_std_s': 0.005804552103140353}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for dev in DEVICES:\n",
    "    for dt in DTYPES:\n",
    "        try:\n",
    "            with tf.device(dev):\n",
    "                A = tf.random.normal((MATMUL_N, MATMUL_N), dtype=dt)\n",
    "                B = tf.random.normal((MATMUL_N, MATMUL_N), dtype=dt)\n",
    "                @tf.function(jit_compile=False)\n",
    "                def run():\n",
    "                    return tf.linalg.matmul(A, B)\n",
    "                times = benchmark_op(run)\n",
    "                res = {\n",
    "                    'workload': 'matmul',\n",
    "                    'device': dev,\n",
    "                    'dtype': str(dt.name),\n",
    "                    'iters': len(times),\n",
    "                    'time_median_s': median(times),\n",
    "                    'time_mean_s': float(np.mean(times)),\n",
    "                    'time_std_s': float(np.std(times)),\n",
    "                }\n",
    "                results.append(res)\n",
    "                print(res)\n",
    "                # free memory\n",
    "                del A, B\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP matmul] {dev} {dt.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload 2: 2D Convolution\n",
    "A single `tf.nn.conv2d` forward pass on a synthetic image batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP conv2d] /CPU:0 float32: module 'tensorflow.experimental' has no attribute 'async_wait'\n",
      "[SKIP conv2d] /CPU:0 float64: module 'tensorflow.experimental' has no attribute 'async_wait'\n"
     ]
    }
   ],
   "source": [
    "for dev in DEVICES:\n",
    "    for dt in DTYPES:\n",
    "        try:\n",
    "            with tf.device(dev):\n",
    "                x = tf.random.normal((CONV_B, CONV_H, CONV_W, CONV_CIN), dtype=dt)\n",
    "                w = tf.random.normal((KERNEL, KERNEL, CONV_CIN, CONV_COUT), dtype=dt)\n",
    "                @tf.function(jit_compile=False)\n",
    "                def run():\n",
    "                    return tf.nn.conv2d(x, w, strides=1, padding='SAME')\n",
    "                times = benchmark_op(run)\n",
    "                res = {\n",
    "                    'workload': 'conv2d',\n",
    "                    'device': dev,\n",
    "                    'dtype': str(dt.name),\n",
    "                    'iters': len(times),\n",
    "                    'time_median_s': median(times),\n",
    "                    'time_mean_s': float(np.mean(times)),\n",
    "                    'time_std_s': float(np.std(times)),\n",
    "                }\n",
    "                results.append(res)\n",
    "                print(res)\n",
    "                del x, w\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP conv2d] {dev} {dt.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload 3: Dense Network Inference\n",
    "A simple Keras MLP (no training), forward pass only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP dense_infer] /CPU:0 float32: module 'tensorflow.experimental' has no attribute 'async_wait'\n",
      "[SKIP dense_infer] /CPU:0 float64: module 'tensorflow.experimental' has no attribute 'async_wait'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "for dev in DEVICES:\n",
    "    for dt in DTYPES:\n",
    "        try:\n",
    "            with tf.device(dev):\n",
    "                # Build model with explicit dtype\n",
    "                inputs = keras.Input(shape=(DENSE_IN,), dtype=dt)\n",
    "                x = inputs\n",
    "                for _ in range(DENSE_L):\n",
    "                    x = layers.Dense(DENSE_H, activation='relu', dtype=dt)(x)\n",
    "                outputs = layers.Dense(DENSE_OUT, dtype=dt)(x)\n",
    "                model = keras.Model(inputs, outputs)\n",
    "                # Create input batch\n",
    "                batch = tf.random.normal((DENSE_B, DENSE_IN), dtype=dt)\n",
    "                @tf.function(jit_compile=False)\n",
    "                def run():\n",
    "                    return model(batch, training=False)\n",
    "                times = benchmark_op(run)\n",
    "                res = {\n",
    "                    'workload': 'dense_infer',\n",
    "                    'device': dev,\n",
    "                    'dtype': str(dt.name),\n",
    "                    'iters': len(times),\n",
    "                    'time_median_s': median(times),\n",
    "                    'time_mean_s': float(np.mean(times)),\n",
    "                    'time_std_s': float(np.std(times)),\n",
    "                }\n",
    "                results.append(res)\n",
    "                print(res)\n",
    "                del model, batch\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP dense_infer] {dev} {dt.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary table and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results collected (likely due to missing TensorFlow ops/devices).\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "if not df.empty:\n",
    "    display(df.sort_values(['workload','device','dtype']))\n",
    "    # Save CSV\n",
    "    out_csv = 'tf_benchmark_results.csv'\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print('Saved results to', out_csv)\n",
    "else:\n",
    "    print('No results collected (likely due to missing TensorFlow ops/devices).')\n",
    "\n",
    "# Simple bar chart of median time\n",
    "if not df.empty:\n",
    "    key = 'time_median_s'\n",
    "    labels = [f\"{w}\\n{d}\\n{t}\" for w,d,t in zip(df['workload'], df['device'], df['dtype'])]\n",
    "    vals = df[key].values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(labels, vals)\n",
    "    plt.ylabel('Median time per iter (s)')\n",
    "    plt.title('TensorFlow float32 vs float64 on CPU/GPU')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips to get clearer results\n",
    "- Close other heavy apps and re-run.\n",
    "- Increase `ITERS` for more stable medians.\n",
    "- Increase problem sizes if you have plenty of RAM/VRAM.\n",
    "- On Colab, make sure you selected the **GPU** runtime (not TPU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-DL-base310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
